# Quick Start: HAWK-AI with Open WebUI

This guide helps you quickly start the integrated HAWK-AI + Open WebUI system.

## 🚀 Quick Start (3 Steps)

### Step 1: Start HAWK-AI Backend
```bash
cd /home/arthurspriet/HAWK-AI
python3 api_server.py
```

**Expected output:**
```
🚀 Starting HAWK-AI API Server...
📋 Registering agents...
🔧 Initializing orchestrator...
✅ HAWK-AI ready!
INFO:     Uvicorn running on http://127.0.0.1:8000
```

**Verify:** Open http://127.0.0.1:8000/docs in your browser

---

### Step 2: Start Open WebUI Frontend
Open a **new terminal**:
```bash
cd /home/arthurspriet/hawk-ai-webchat
npm run dev
```

**Expected output:**
```
  VITE v4.x.x  ready in xxx ms

  ➜  Local:   http://localhost:5173/
  ➜  Network: use --host to expose
```

**Verify:** Open http://localhost:5173 in your browser

---

### Step 3: Test the Integration
1. Open http://localhost:5173 in your browser
2. Click "New Chat" or start chatting
3. Send a message: **"Analyze current tensions in Sudan"**
4. Watch for HAWK-AI processing

**Expected behavior:**
- Browser console (F12) shows `[HAWK-AI]` logs
- Backend terminal shows `[CHAT] Incoming...`
- Response appears in chat UI
- Response should come from HAWK-AI agents (not Ollama)

---

## ✅ Success Indicators

### Browser Console (Press F12)
You should see:
```
[HAWK-AI] Intercepting chat request
[HAWK-AI] Model: supervisor
[HAWK-AI] Query: Analyze current tensions in Sudan...
[HAWK-AI] Response received from supervisor, analyst, geo
[HAWK-AI] Duration: 2.5 seconds
```

### HAWK-AI Backend Terminal
You should see:
```
[CHAT] Incoming (supervisor): Analyze current tensions in Sudan
```

### Open WebUI
- Message appears in chat
- Response is generated by HAWK-AI (not Ollama)
- No errors displayed

---

## ⚠️ Troubleshooting

### Problem: "Connection refused" or CORS error
**Solution:**
```bash
# Ensure HAWK-AI backend is running on port 8000
curl http://127.0.0.1:8000/health
# Should return: {"status": "healthy"}
```

### Problem: Open WebUI shows error message
**Check:**
1. Browser console (F12) for `[HAWK-AI]` error logs
2. HAWK-AI backend terminal for errors
3. Both services are running

### Problem: No `[HAWK-AI]` logs in console
**This means the integration isn't active.**
**Verify:**
```bash
cd /home/arthurspriet/hawk-ai-webchat
grep "hawkai" src/lib/components/chat/Chat.svelte
# Should show: import { generateOpenAIChatCompletion } from '$lib/apis/hawkai';
```

### Problem: Backend not responding
**Check:**
```bash
# Test backend directly
curl -X POST http://127.0.0.1:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Test message"}'
```

**Expected response:**
```json
{
  "response": "...",
  "status": "success",
  "duration": 1.5,
  "agents_used": ["supervisor"],
  "session_id": "...",
  "timestamp": "..."
}
```

---

## 📝 What Changed

### Files Modified

1. **Created:** `/home/arthurspriet/hawk-ai-webchat/src/lib/apis/hawkai/index.ts`
   - New API module for HAWK-AI integration
   - Provides `generateOpenAIChatCompletion()` wrapper

2. **Modified:** `/home/arthurspriet/hawk-ai-webchat/src/lib/components/chat/Chat.svelte`
   - **Line 64**: Changed import from `openai` to `hawkai`

3. **Modified:** `/home/arthurspriet/HAWK-AI/api_server.py`
   - **Lines 32-47**: Updated CORS to allow Open WebUI ports

### Total Changes
- **3 files** modified
- **1 new file** created
- **1 line** changed in Chat.svelte (the import)

---

## 🔄 Switching Back to Ollama

If you need to revert to Ollama:

```bash
cd /home/arthurspriet/hawk-ai-webchat
# Edit src/lib/components/chat/Chat.svelte line 64:
# Change: import { generateOpenAIChatCompletion } from '$lib/apis/hawkai';
# To:     import { generateOpenAIChatCompletion } from '$lib/apis/openai';
```

Then restart the frontend:
```bash
# Press Ctrl+C to stop npm run dev
npm run dev
```

---

## 📊 Test Examples

### Example 1: Geopolitical Analysis
```
Analyze current tensions in Sudan
```

Expected: Response from analyst_agent and geo_agent

### Example 2: Historical Context
```
Provide historical context on Nigeria's Boko Haram insurgency
```

Expected: Response from analyst_agent with historical data

### Example 3: Web Research
```
Find latest news on tensions in the Sahel region
```

Expected: Response from search_agent with recent articles

### Example 4: Simple Query
```
What is HAWK-AI?
```

Expected: Response from supervisor_agent

---

## 🔧 Development Mode

### Hot Reload
Both services support hot reload:
- **Frontend**: Vite auto-reloads on file changes
- **Backend**: Uvicorn auto-reloads on Python file changes

### Debugging
Enable debug logs:

**Frontend (browser console):**
- Already enabled - check for `[HAWK-AI]` prefix

**Backend:**
```bash
# Check logs
tail -f /home/arthurspriet/HAWK-AI/logs/hawk_ai.log
```

---

## 📚 More Information

- **Full Documentation**: See `OPENWEBUI_INTEGRATION.md`
- **API Docs**: http://127.0.0.1:8000/docs (when backend running)
- **Frontend Source**: `/home/arthurspriet/hawk-ai-webchat/src/lib/apis/hawkai/`
- **Backend Source**: `/home/arthurspriet/HAWK-AI/api_server.py`

---

## ⚡ One-Command Start (Optional)

Create a start script for convenience:

```bash
# Create start script
cat > /home/arthurspriet/start_hawk_ui.sh << 'EOF'
#!/bin/bash
echo "🚀 Starting HAWK-AI Integration..."

# Start backend
cd /home/arthurspriet/HAWK-AI
python3 api_server.py &
BACKEND_PID=$!

# Wait for backend to be ready
sleep 5

# Start frontend
cd /home/arthurspriet/hawk-ai-webchat
npm run dev &
FRONTEND_PID=$!

echo "✅ Started:"
echo "   - HAWK-AI Backend (PID: $BACKEND_PID) on http://127.0.0.1:8000"
echo "   - Open WebUI Frontend (PID: $FRONTEND_PID) on http://localhost:5173"
echo ""
echo "Press Ctrl+C to stop all services"

# Wait for Ctrl+C
trap "kill $BACKEND_PID $FRONTEND_PID; exit" INT
wait
EOF

# Make executable
chmod +x /home/arthurspriet/start_hawk_ui.sh

# Run it
./start_hawk_ui.sh
```

---

## 🎉 Success!

If you see the `[HAWK-AI]` logs and get responses in the chat, the integration is working correctly!

All chat messages from Open WebUI now go to your HAWK-AI SupervisorAgent, which orchestrates the response using the appropriate specialist agents.

